{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579d13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"expandable_segments not supported on this platform\")\n",
    "os.environ[\"MLFLOW_LOCK_MODEL_DEPENDENCIES\"] = \"true\"\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.__version__)\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214868c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "import optuna\n",
    "import mlflow\n",
    "\n",
    "from spikingjelly.clock_driven import functional\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "import traceback\n",
    "\n",
    "import importlib\n",
    "import dtypeconvert\n",
    "import snn_model\n",
    "import ravdess_dataset\n",
    "importlib.reload(ravdess_dataset)\n",
    "importlib.reload(snn_model)\n",
    "importlib.reload(dtypeconvert)\n",
    "from snn_model import EmotionSNN\n",
    "from ravdess_dataset import RAVDESSDataset\n",
    "from dtypeconvert import convert_dataset_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14209b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/24 23:51:41 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/09/24 23:51:41 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/Marc/Desktop/Programming/Main Project/snn/audio-snn/mlruns/1', creation_time=1758737404883, experiment_id='1', last_update_time=1758737404883, lifecycle_stage='active', name='SNN_audio-experiment-study', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///snn.db\")\n",
    "mlflow.set_experiment(\"SNN_audio-experiment-study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6db225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5842364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell early in the notebook\n",
    "import platform\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def log_environment():\n",
    "    \"\"\"Log complete environment for reproducibility\"\"\"\n",
    "    env_info = {\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"spikingjelly_version\": getattr(functional, '__version__', 'unknown'),\n",
    "        \"mlflow_version\": mlflow.__version__,\n",
    "        \"optuna_version\": optuna.__version__,\n",
    "    }\n",
    "    \n",
    "    # Log complete environment as JSON artifact\n",
    "    mlflow.log_text(json.dumps(env_info, indent=2), \"environment.json\")\n",
    "    \n",
    "    # Log key info as MLflow tags (for filtering/searching)\n",
    "    mlflow.set_tag(\"env_pytorch_version\", env_info[\"pytorch_version\"])\n",
    "    mlflow.set_tag(\"env_cuda_version\", str(env_info[\"cuda_version\"]))\n",
    "    mlflow.set_tag(\"env_cuda_available\", str(env_info[\"cuda_available\"]))\n",
    "    mlflow.set_tag(\"env_gpu_name\", env_info[\"gpu_name\"])\n",
    "    mlflow.set_tag(\"env_platform\", env_info[\"platform\"])\n",
    "\n",
    "    return env_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c34d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# PyTorch deterministic flags for reproducibility (may slow training)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd05715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 864, Validation size: 288, Test size: 288\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"Audio_Speech_Actors_01-24\"\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = RAVDESSDataset(root_dir=dataset_root, T=50, augment_prob=0.7)\n",
    "\n",
    "train_size = int(0.6 * len(full_dataset))\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "# Log split indices for exact reproduction\n",
    "train_indices = train_dataset.indices\n",
    "val_indices = val_dataset.indices\n",
    "test_indices = test_dataset.indices\n",
    "\n",
    "train_dataset = convert_dataset_dtype(train_dataset, torch.float16)\n",
    "val_dataset = convert_dataset_dtype(val_dataset, torch.float16)\n",
    "test_dataset = convert_dataset_dtype(test_dataset, torch.float16)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a24d32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 128, 400]) torch.float16\n",
      "torch.Size([]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "print(x.shape, x.dtype)\n",
    "print(y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c57bece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 0.0 \n",
      "max: 1.0 \n",
      "mean: 0.0919189453125\n",
      "y: tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# quick checks for what train_dataset returns\n",
    "print(f\"min: {x.min().item()} \\nmax: {x.max().item()} \\nmean: {x.mean().item()}\")\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578440a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "\n",
    "\n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Update aggregates\n",
    "        batch_size = target.size(0)\n",
    "        total_loss += loss.item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == target).sum().item()\n",
    "\n",
    "        # Reset SNN states to avoid carryover between batches\n",
    "        functional.reset_net(model)\n",
    "\n",
    "    # Safeguards\n",
    "    num_batches = max(1, len(val_loader))\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100.0 * (total_correct / max(1, total_samples))\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f71fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    device,\n",
    "    patience=10,\n",
    "    trial=None,\n",
    "    min_delta=0.001,\n",
    "    ckpt_dir=\"checkpoints\",\n",
    "    grad_clip_max_norm: Optional[float] = None,\n",
    "    report_intermediate = True\n",
    "):\n",
    "    \n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    \n",
    "    # advanced optimizer setup\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=learning_rate,\n",
    "                                 weight_decay=0.01,\n",
    "                                 betas=(0.9, 0.999)\n",
    "                    )\n",
    "\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "        min_lr=1e-7,\n",
    "        threshold=min_delta\n",
    "    )\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    best_model_state = None\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            # Combined loss with L2 regularization\n",
    "            ce_loss = criterion(output, target)\n",
    "            l2_loss = model.get_l2_loss()\n",
    "            loss = ce_loss + l2_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            if grad_clip_max_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            functional.reset_net(model)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            train_acc += pred.eq(target).sum().item()\n",
    "            \n",
    "            # printing batch summary\n",
    "            batch_acc = 100.0 * pred.eq(target).sum().item() / len(target)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}: \"\n",
    "                f\"Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.2f}%\")\n",
    "            \n",
    "        # ------- Validate -------\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # ------- Scheduler step -------\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # ------- Calculate metrics -------\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = 100.0 * train_acc / len(train_loader.dataset)\n",
    "        \n",
    "        train_accs.append(epoch_train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # ------- Optuna Reporting -------\n",
    "        if trial is not None and report_intermediate:\n",
    "            trial.report(val_loss, epoch)\n",
    "            trial.set_user_attr(f\"val_acc_epoch_{epoch}\", val_acc)\n",
    "\n",
    "        # ------- Track best -------\n",
    "        val_improved = val_acc > best_val_acc + min_delta\n",
    "        loss_improved = val_loss < best_val_loss - min_delta\n",
    "        \n",
    "        # Update best metrics\n",
    "        if val_improved:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "            \n",
    "        if loss_improved:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        # ------- MLFlow Logging -------\n",
    "        mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", epoch_train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "        mlflow.log_metric(\"best_val_acc\", best_val_acc, step=epoch)\n",
    "        mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "        \n",
    "        # ------ optuna pruning hook ------\n",
    "        if trial is not None:\n",
    "            if trial.should_prune():\n",
    "                print(f\"Trial {trial.number} pruned at epoch {epoch}\")\n",
    "                if best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                raise optuna.TrialPruned()\n",
    "          \n",
    "        # -------- Print epoch summary -------\n",
    "        if trial is None:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "                f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, '\n",
    "                f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                f'Best Val Acc: {best_val_acc:.2f}%, '\n",
    "                f'LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "\n",
    "    # Load the best weights seen during this call\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    return best_val_acc, best_val_loss, train_accs, val_accs, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf179cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function\n",
    "def objective(trial, parent_run_id):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "       \n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "            'conv1_channels': trial.suggest_int('conv1_channels', 16, 64, step=8),\n",
    "            'conv2_channels': trial.suggest_int('conv2_channels', 32, 128, step=16),\n",
    "            'fc1_units': trial.suggest_int('fc1_units', 64, 256, step=32),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [2, 4]),\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "            \n",
    "            # model specific\n",
    "            \"v_threshold\" : trial.suggest_float(\"v_threshold\", 0.2, 1.0),\n",
    "            \"tau\" : trial.suggest_float(\"tau\", 1.0, 4.0),\n",
    "            'surrogate_func': trial.suggest_categorical('surrogate_func', ['Sigmoid', 'ATan']),\n",
    "            'T_steps': trial.suggest_int('T_steps', 50, 200, step=25),\n",
    "            }\n",
    "        \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], \n",
    "                              shuffle=True, drop_last=True,\n",
    "                              num_workers=0,pin_memory=True, \n",
    "                              )\n",
    "    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], \n",
    "                            shuffle=False, drop_last=False, \n",
    "                            num_workers=0,pin_memory=True, \n",
    "                            )\n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\",nested=True, parent_run_id=parent_run_id):\n",
    "        \n",
    "        # Set different random seed for each trial\n",
    "        trial_seed = SEED + trial.number\n",
    "        torch.manual_seed(trial_seed)\n",
    "        torch.cuda.manual_seed_all(trial_seed)\n",
    "        \n",
    "        mlflow.set_tag(\"phase\", \"optuna\")\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log data splits for reproducibility\n",
    "        mlflow.log_text(json.dumps({\n",
    "            \"train_indices\": train_indices,\n",
    "            \"val_indices\": val_indices,\n",
    "            \"test_indices\": test_indices\n",
    "        }, indent=2), \"data_splits.json\")\n",
    "    \n",
    "        # Create model with suggested parameters\n",
    "        model = EmotionSNN(\n",
    "            num_classes=8,\n",
    "            conv1_channels=params['conv1_channels'],\n",
    "            conv2_channels=params['conv2_channels'], \n",
    "            fc1_units=params['fc1_units'],\n",
    "            surrogate_func=params['surrogate_func'],\n",
    "            dropout_rate=params['dropout_rate']\n",
    "        ).to(device).half()\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Train model (shorter for hyperparameter search)\n",
    "        try:\n",
    "            best_val_acc, best_val_loss, train_accs, val_accs, train_losses, val_losses = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                num_epochs=20,  # Shorter for optimization\n",
    "                learning_rate=params['learning_rate'],\n",
    "                device=device,\n",
    "                patience=15,\n",
    "                trial=trial,\n",
    "                grad_clip_max_norm=1.0,\n",
    "            )\n",
    "\n",
    "            return best_val_loss, best_val_acc\n",
    "        \n",
    "        except optuna.exceptions.TrialPruned:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(\"Trial failed, returning high loss\")\n",
    "            return float('inf'), 0  # Return poor score for failed trials\n",
    "        \n",
    "        finally:\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a533178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_optimization(n_trials=50):\n",
    "    # Run Optuna hyperparameter optimization\n",
    "\n",
    "    # Create study\n",
    "    sampler = optuna.samplers.TPESampler(n_startup_trials=10, n_ei_candidates=24, multivariate=True, group=True)\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10, interval_steps=5)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        directions=[\"minimize\", \"maximize\"],\n",
    "        study_name=\"snn_emotion_recognition\",\n",
    "        storage=\"sqlite:///snn_optuna.db\",  # Persist results\n",
    "        load_if_exists=True,\n",
    "        pruner=pruner,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    print(f\"Will run {n_trials} trials\")\n",
    "\n",
    "    # callback: print + optuna pruning only\n",
    "    def stop_callback(study, trial):\n",
    "        try:\n",
    "            best_val = study.best_value\n",
    "            print(f\"Trial {trial.number} completed. Best validation loss so far: {best_val:.2f}\")\n",
    "        except Exception:\n",
    "            print(f\"Trial {trial.number} completed.\")\n",
    "\n",
    "    with mlflow.start_run(run_name=\"snn_optuna_sweep\") as parent_run:\n",
    "        parent_run_id = parent_run.info.run_id\n",
    "        \n",
    "        mlflow.set_tag(\"phase\", \"optuna_sweep\")\n",
    "        mlflow.set_tag(\"model\", \"EmotionSNN\")\n",
    "        mlflow.log_artifact(\"requirements.txt\")\n",
    "\n",
    "        study.optimize(partial(objective, parent_run_id=parent_run_id), n_trials=n_trials, timeout=7200, callbacks=[stop_callback])\n",
    "\n",
    "        df = study.trials_dataframe()\n",
    "        df.to_csv(\"study_trials.csv\", index=False)\n",
    "        mlflow.log_artifact(\"study_trials.csv\", artifact_path=\"optuna\")\n",
    "        \n",
    "        # Save best parameters to a JSON file\n",
    "        with open(\"best_params.json\", \"w\") as f:\n",
    "            json.dump(study.best_params, f, indent=2)\n",
    "\n",
    "        # Log the file as an artifact in MLflow\n",
    "        mlflow.log_artifact(\"best_params.json\", artifact_path=\"optuna\")\n",
    "        \n",
    "        mlflow.log_text(json.dumps({\"best_value\": study.best_value}, indent=2), \"optuna/best_value.json\")\n",
    "        return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea957e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 MB\n",
      "Reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03597c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Warmup step: Training with baseline hyperparameters ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc\\anaconda3\\envs\\snn_gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Batch 1/216: Loss: 2.4630, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 2/216: Loss: nan, Batch Acc: 25.00%\n",
      "Epoch 1/2, Batch 3/216: Loss: nan, Batch Acc: 50.00%\n",
      "Epoch 1/2, Batch 4/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 5/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 6/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 7/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 8/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 9/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 10/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 11/216: Loss: nan, Batch Acc: 0.00%\n",
      "Epoch 1/2, Batch 12/216: Loss: nan, Batch Acc: 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m\n\u001b[0;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m EmotionSNN(\n\u001b[0;32m     25\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     26\u001b[0m     conv1_channels\u001b[38;5;241m=\u001b[39mbaseline_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv1_channels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mbaseline_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[0;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Just a few epochs for warmup\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_clip_max_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Warmup complete ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 60\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate, device, patience, trial, min_delta, ckpt_dir, grad_clip_max_norm, report_intermediate)\u001b[0m\n\u001b[0;32m     57\u001b[0m l2_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_l2_loss()\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m l2_loss\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Gradient clipping to prevent exploding gradients\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_clip_max_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\snn_gpu\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\snn_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\snn_gpu\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"=== Warmup step: Training with baseline hyperparameters ===\")\n",
    "baseline_params = {\n",
    "    'conv1_channels': 32,\n",
    "    'conv2_channels': 64,\n",
    "    'fc1_units': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 4,\n",
    "    'dropout_rate': 0.3,\n",
    "    'surrogate_func': 'Sigmoid',\n",
    "    'T_steps': 50,\n",
    "    'v_threshold': 0.5,\n",
    "    'tau': 2.0,\n",
    "}\n",
    "train_loader = DataLoader(train_dataset, batch_size=baseline_params['batch_size'], \n",
    "                              shuffle=True, drop_last=True,\n",
    "                              num_workers=0,pin_memory=True, \n",
    "                              )\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=baseline_params['batch_size'], \n",
    "                            shuffle=False, drop_last=False, \n",
    "                            num_workers=0,pin_memory=True, \n",
    "                            )\n",
    "\n",
    "model = EmotionSNN(\n",
    "    num_classes=8,\n",
    "    conv1_channels=baseline_params['conv1_channels'],\n",
    "    conv2_channels=baseline_params['conv2_channels'],\n",
    "    fc1_units=baseline_params['fc1_units'],\n",
    "    surrogate_func=baseline_params['surrogate_func'],\n",
    "    dropout_rate=baseline_params['dropout_rate']\n",
    ").to(device).half()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=2,  # Just a few epochs for warmup\n",
    "    learning_rate=baseline_params['learning_rate'],\n",
    "    device=device,\n",
    "    patience=2,\n",
    "    trial=None,\n",
    "    grad_clip_max_norm=1.0,\n",
    ")\n",
    "print(\"=== Warmup complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d623e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optuna hyperparameter optimization\n",
    "print(\"=== Option 2: Hyperparameter Optimization ===\")\n",
    "study = run_hyperparameter_optimization(n_trials=100)  # Start with 100 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5d708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
